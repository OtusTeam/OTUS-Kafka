0) Запускаем демонстрацию

0.1) Запускаем сервисы
docker compose up -d

0.2) Проверяем контейнеры
docker compose ps -a

0.3) Проверям логи Kafka Connect
docker logs -f connect
^C

0.4) Открываем два терминала


1) Основы ksqlDB

1.1) В первом терминале проверяем список топиков
docker exec kafka1 kafka-topics --list --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092

1.2) Запускаем ksqlDB CLI
docker exec -ti ksqldb-cli ksql http://ksqldb-server:8088

1.3) Устанавливаем чтение с начала темы
SET 'auto.offset.reset' = 'earliest';

1.4) Проверяем топики
SHOW TOPICS;

1.5) Создаём поток, который читает топик users
CREATE STREAM usersStream (ROWKEY INT KEY, USERNAME VARCHAR) WITH (KAFKA_TOPIC='users', VALUE_FORMAT='JSON', PARTITIONS=3);

1.6) Работа с потоками
SHOW STREAMS;
SHOW STREAMS EXTENDED;
DESCRIBE usersStream;
DESCRIBE usersStream EXTENDED;

1.7) Добавим записи в поток
INSERT INTO usersStream VALUES (1, 'Alex');
INSERT INTO usersStream VALUES (2, 'Barbara');
INSERT INTO usersStream VALUES (3, 'Carl');
INSERT INTO usersStream VALUES (4, 'Fiona');

1.8) Прочитаем записи (pull-запрос) в потоке
SELECT * FROM usersStream;

1.9) Прочитаем записи в топике
PRINT users FROM BEGINNING;

1.10) Создадим push-запрос
SELECT 'Hello, ' + USERNAME AS GREETING FROM usersStream EMIT CHANGES;

1.11) Во втором терминале добавляем записи в поток
docker exec -ti ksqldb-cli ksql http://ksqldb-server:8088
INSERT INTO usersStream VALUES (5, 'Mary');
INSERT INTO usersStream VALUES (6, 'Garry');
INSERT INTO usersStream VALUES (7, 'Ann');

1.12) В первом терминале посмотрим на метаданные
^C
SELECT ROWTIME, ROWOFFSET, ROWPARTITION, * FROM usersStream EMIT CHANGES;

1.13) Во втором терминале работаем с запросами
LIST QUERIES;
SHOW QUERIES EXTENDED;

1.14) Работаем с функциями
SHOW FUNCTIONS;
DESCRIBE FUNCTION UUID;
^D

1.15) Во первом терминале завершаем push-запрос
^C

## Непрерывные запросы

1.16) В первом терминале создаём первый поток
CREATE STREAM PurchaseStream (id INT KEY, product VARCHAR, left_ts VARCHAR) WITH (KAFKA_TOPIC='PurchaseTopic', VALUE_FORMAT='JSON', TIMESTAMP='left_ts', TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssX', PARTITIONS=3);

1.17) Создаём второй поток
CREATE STREAM PaymentStream (id INT KEY, purchaseId INT, status VARCHAR, right_ts VARCHAR) WITH (KAFKA_TOPIC='PaymentTopic', VALUE_FORMAT='JSON', TIMESTAMP='right_ts', TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssX', PARTITIONS=3);

1.18) Создаём CSAS - Create Stream As Select
CREATE STREAM PaymentPurchaseStream
WITH (KAFKA_TOPIC = 'PaymentPurchaseTopic', VALUE_FORMAT='JSON')
AS SELECT l.id AS purchaseId, l.product, r.status
    FROM PurchaseStream l
    INNER JOIN PaymentStream r
    WITHIN 7 DAYS
    GRACE PERIOD 24 HOURS
    ON l.id = r.purchaseId
    EMIT CHANGES;

1.19) Добавим записи в потоки
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (1, 'kettle', '2022-01-29T06:01:18Z');
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (2, 'grill' , '2022-01-29T17:02:20Z');
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (3, 'toaster', '2022-01-29T13:44:10Z');
INSERT INTO PurchaseStream (id, product, left_ts) VALUES (4, 'hair dryer', '2022-01-29T11:58:25Z');

INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (101, 1, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (103, 3, 'OK', '2022-01-29T13:54:10Z');
INSERT INTO PaymentStream (id, purchaseId, status, right_ts) VALUES (104, 4, 'OK', '2022-01-29T12:08:25Z');

1.20) Проверяем объединённый поток
SELECT * FROM PaymentPurchaseStream;


## Агрегирование

1.22) Создаём поток
CREATE STREAM OrderStream (ID INT KEY, status VARCHAR, ts VARCHAR) WITH (KAFKA_TOPIC='OrderTopic', VALUE_FORMAT='JSON', TIMESTAMP='ts', TIMESTAMP_FORMAT='yyyy-MM-dd''T''HH:mm:ssX', PARTITIONS=3);

1.23) Создаём CTAS - Create Table As Select
CREATE TABLE OrderTable AS
    SELECT id, count(*) as count
    FROM OrderStream
    GROUP BY id
    EMIT CHANGES;

1.24) Добавим записи в поток
INSERT INTO OrderStream VALUES (2, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO OrderStream VALUES (3, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO OrderStream VALUES (2, 'OK', '2022-01-29T13:54:10Z');
INSERT INTO OrderStream VALUES (4, 'OK', '2022-01-29T12:08:25Z');
INSERT INTO OrderStream VALUES (1, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO OrderStream VALUES (2, 'OK', '2022-01-29T06:11:18Z');
INSERT INTO OrderStream VALUES (2, 'OK', '2022-01-29T13:54:10Z');
INSERT INTO OrderStream VALUES (3, 'OK', '2022-01-29T13:54:10Z');

1.25) Посчитаем количество записей по каждому id
SELECT * FROM OrderTable;


2) ksqlDB + Kafka Connect, Вариант 1

2.1) Во втором терминале проверяем статус и плагины коннекторов
curl http://localhost:8083 | jq
curl http://localhost:8083/connector-plugins | jq

2.2) Проверяем список топиков
docker exec kafka1 kafka-topics --list --bootstrap-server kafka1:9092,kafka2:9092,kafka3:9092

2.3) Создадим таблицу в PostgreSQL и запишем в неё данные
docker exec -ti postgres psql -U postgres
\dt
CREATE TABLE titles (id SERIAL PRIMARY KEY, title VARCHAR(120));
INSERT INTO titles (title) values ('Stranger Things');
INSERT INTO titles (title) values ('Black Mirror');
INSERT INTO titles (title) values ('The Office');
SELECT * FROM titles;
\q

2.4) В первом терминале создаём коннектор postgres-source
CREATE SOURCE CONNECTOR `postgres-source` WITH (
    "connector.class" = 'io.confluent.connect.jdbc.JdbcSourceConnector',
    "connection.url" = 'jdbc:postgresql://postgres:5432/postgres?user=postgres&password=password',
    "mode" = 'incrementing',
    "incrementing.column.name" = 'id',
    "table.whitelist" = 'titles',
    "topic.prefix" = 'postgres.',
    "key" = 'id');

2.5) Выводим список коннекторов
SHOW CONNECTORS;

2.6) Получаем описание коннектора inventory-connector
DESCRIBE CONNECTOR `postgres-source`;

2.7) Проверяем топики
SHOW TOPICS;

2.8) Выведем содержимое топика
PRINT `postgres.titles` FROM BEGINNING;
^C

2.9) Создаём поток, который читает топик postgres.titles
CREATE STREAM titles WITH (kafka_topic = 'postgres.titles', value_format = 'avro');

2.10) Выведем описание потока
DESCRIBE titles EXTENDED;

2.11) Прочитаем записи (push-запрос)
SELECT * FROM titles EMIT CHANGES;

2.12) Во втором терминале дабавляем записи
INSERT INTO titles (title) values ('Four');
INSERT INTO titles (title) values ('Five');
INSERT INTO titles (title) values ('Six');
SELECT * FROM titles;

2.13) Во первом терминале завершаем push-запрос
^C


3) ksqlDB + Kafka Connect, Вариант 2

3.1) Во втором терминале создадим таблицу в PostgreSQL и запишем в неё данные
CREATE TABLE customers (id INT PRIMARY KEY, name TEXT, age INT);
INSERT INTO customers (id, name, age) VALUES (5, 'Fred', 34);
INSERT INTO customers (id, name, age) VALUES (7, 'Sue', 25);
INSERT INTO customers (id, name, age) VALUES (2, 'Bill', 51);
SELECT * FROM customers;

3.2) В первом терминале создаём коннектор customers_source
CREATE SOURCE CONNECTOR `customers_source` WITH (
    'connector.class' = 'io.debezium.connector.postgresql.PostgresConnector',
    'database.hostname' = 'postgres',
    'database.port' = '5432',
    'database.user' = 'postgres',
    'database.password' = 'password',
    'database.dbname' = 'postgres',
    'database.server.name' = 'postgres',
    'table.include.list' = 'public.customers',
    'topic.prefix' = 'postgres',
    'key.converter' = 'org.apache.kafka.connect.json.JsonConverter',
    'key.converter.schemas.enable' = 'false',
    'transforms' = 'unwrap',
    'transforms.unwrap.type' = 'io.debezium.transforms.ExtractNewRecordState',
    'transforms.unwrap.drop.tombstones' = 'true',
    'transforms.unwrap.delete.handling.mode' = 'rewrite',
    'plugin.name' = 'pgoutput',
    'tasks.max' = '1'
);

3.3) Выводим список коннекторов
SHOW CONNECTORS;

3.4) Получаем описание коннектора inventory-connector
DESCRIBE CONNECTOR `customers_source`;

3.5) Проверяем топики
SHOW TOPICS;

3.6) Выведем содержимое топика
PRINT `postgres.public.customers` FROM BEGINNING;
^C

3.7) Создаём поток
CREATE STREAM customers WITH (kafka_topic = 'postgres.public.customers', value_format = 'avro');

3.8) Выведем содержимое потока
SELECT * FROM customers EMIT CHANGES;

3.9) Во втором терминале дабавляем, изменяем и удаляем записи
INSERT INTO customers (id, name, age) VALUES (3, 'Ann', 18);
UPDATE customers set age = 35 WHERE id = 5;
DELETE FROM customers WHERE id = 2;
SELECT * FROM customers;
\q

3.10) В первом терминале создаём таблицу
^C
CREATE TABLE customers_by_key AS
    SELECT id,
           latest_by_offset(name, false) AS name,
           latest_by_offset(age, false) AS age
    FROM customers
    GROUP BY id
    EMIT CHANGES;

3.11) Создадим запрос к таблице
SELECT * FROM customers_by_key;

3.12) Создаём таблицу
CREATE TABLE customers_by_key_2 AS
    SELECT id,
           latest_by_offset(name, true) AS name,
           latest_by_offset(age, true) AS age
    FROM customers
    GROUP BY id
    EMIT CHANGES;

3.13) Создадим запрос к таблице
SELECT * FROM customers_by_key_2;
^D


4) Завершаем работу
docker compose down
docker container prune -f
docker volume prune -f
docker network prune -f
